# Cognee 产品说明文档

## 一、产品概述

### 1.1 产品定位

**Cognee** 是一个开源的人工智能记忆平台，致力于将原始数据转换为持久化和动态的 AI 智能体记忆系统。它结合了向量搜索和图数据库技术，使文档既能通过语义搜索，又能通过关系连接，为 AI 智能体提供强大的知识记忆能力。

### 1.2 核心价值

- **替代传统 RAG 系统**：用统一的基于图和向量的记忆层替代传统 RAG 架构
- **降低开发成本**：减少开发者工作量和基础设施成本，同时提高质量和精确度
- **数据互联互通**：支持任何类型的数据互联，包括历史对话、文件、图像和音频转录
- **高度可定制**：通过用户定义的任务、模块化管道和内置搜索端点提供高度可定制性

### 1.3 版本信息

- **当前版本**：0.4.1
- **Python 要求**：3.10 - 3.13
- **许可证**：Apache-2.0

---

## 二、核心功能

### 2.1 ECL 管道（Extract, Cognify, Load）

Cognee 使用 ECL 管道替代传统的 RAG 系统：

1. **Extract（提取）**：从各种数据源提取内容
2. **Cognify（认知化）**：将数据转换为结构化知识图谱
3. **Load（加载）**：将处理后的数据加载到存储系统

### 2.2 主要功能模块

#### 2.2.1 数据添加（Add）
- 支持多种数据格式：文本、PDF、图像、音频、代码等
- 支持 30+ 数据源
- 自动文件类型识别和加载器选择
- 增量加载支持
- 数据集管理和组织

#### 2.2.2 认知化处理（Cognify）
将原始数据转换为结构化知识图谱的核心处理步骤：

- **文档分类**：自动识别文档类型和结构
- **文本分块**：将内容分割成语义上有意义的片段
- **实体提取**：使用 LLM 识别关键概念、人物、地点、组织等
- **关系检测**：发现实体之间的连接关系
- **图谱构建**：构建带有嵌入向量的语义知识图谱
- **内容摘要**：创建层次化摘要用于导航

支持自定义图谱模型、自定义提示词、批量处理、后台处理等高级功能。

#### 2.2.3 记忆化增强（Memify）
在已有知识图谱基础上进行增强处理：

- **提取任务**：从图谱或数据中提取子图或特定信息
- **增强任务**：对提取的内容进行进一步处理和关联
- **规则关联**：为代码智能体等场景添加规则关联
- **反馈增强**：基于用户反馈改进答案质量

#### 2.2.4 智能搜索（Search）
提供多种搜索模式：

- **GRAPH_COMPLETION**（默认）：使用完整图谱上下文和 LLM 推理的自然语言问答
- **RAG_COMPLETION**：传统 RAG 方式，基于文档块
- **CHUNKS**：语义匹配的文本片段
- **SUMMARIES**：预生成的层次化摘要
- **CODE**：代码特定搜索，支持语法和语义理解
- **CYPHER**：直接图数据库查询
- **FEELING_LUCKY**：智能选择最合适的搜索类型
- **CHUNKS_LEXICAL**：基于词法的块搜索

#### 2.2.5 数据管理
- **更新（Update）**：更新已添加的数据
- **删除（Delete）**：删除数据集或特定数据
- **同步（Sync）**：本地数据与云端同步
- **数据集管理**：创建、查询、管理数据集

#### 2.2.6 可视化
- 知识图谱可视化
- 网络图展示
- 交互式探索界面

---

## 三、技术架构

### 3.1 系统架构

```
┌─────────────────────────────────────────────────────────┐
│                    FastAPI 后端服务                      │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐            │
│  │   API    │  │   CLI    │  │   MCP    │            │
│  │ 路由层   │  │  命令层  │  │  协议层  │            │
│  └──────────┘  └──────────┘  └──────────┘            │
└─────────────────────────────────────────────────────────┘
                        │
        ┌───────────────┼───────────────┐
        │               │               │
┌───────▼──────┐ ┌──────▼──────┐ ┌──────▼──────┐
│   模块层     │ │  基础设施层  │ │   任务层    │
│ - Graph     │ │ - Databases │ │ - Tasks     │
│ - Retrieval │ │ - LLM       │ │ - Pipelines │
│ - Search    │ │ - Loaders   │ │             │
│ - Users     │ │ - Storage   │ │             │
└──────────────┘ └─────────────┘ └─────────────┘
        │               │               │
        └───────────────┼───────────────┘
                        │
        ┌───────────────▼───────────────┐
        │      数据存储层                │
        │  ┌────────┐  ┌────────┐      │
        │  │ Vector │  │ Graph  │      │
        │  │   DB   │  │   DB   │      │
        │  └────────┘  └────────┘      │
        └───────────────────────────────┘
```

### 3.2 核心模块

#### 3.2.1 API 层（`cognee/api/`）
- **FastAPI 应用**：提供 RESTful API 接口
- **版本化路由**：v1 API 路由组织
- **主要端点**：
  - `/api/v1/add` - 添加数据
  - `/api/v1/cognify` - 认知化处理
  - `/api/v1/memify` - 记忆化增强
  - `/api/v1/search` - 智能搜索
  - `/api/v1/delete` - 删除数据
  - `/api/v1/update` - 更新数据
  - `/api/v1/datasets` - 数据集管理
  - `/api/v1/users` - 用户管理
  - `/api/v1/permissions` - 权限管理
  - `/api/v1/settings` - 配置管理
  - `/api/v1/visualize` - 可视化
  - `/api/v1/sync` - 数据同步
  - `/api/v1/responses` - 响应管理
  - `/api/v1/code-pipeline` - 代码图谱管道

#### 3.2.2 模块层（`cognee/modules/`）
- **graph**：知识图谱构建和管理
- **retrieval**：多种检索器实现
- **search**：搜索功能实现
- **chunking**：文本分块策略
- **memify**：记忆化处理
- **pipelines**：管道执行引擎
- **users**：用户、租户、权限管理
- **ontology**：本体解析和匹配
- **ingestion**：数据摄取
- **visualization**：可视化功能
- **observability**：可观测性

#### 3.2.3 基础设施层（`cognee/infrastructure/`）
- **databases**：
  - **vector**：向量数据库适配器（LanceDB、ChromaDB、PGVector、Neptune Analytics）
  - **graph**：图数据库适配器（Kuzu、Neo4j、Neptune）
  - **relational**：关系数据库（PostgreSQL、SQLite）
  - **cache**：缓存系统（Redis）
- **llm**：LLM 提供商集成（OpenAI、Anthropic、Mistral、Groq 等）
- **loaders**：文件加载器（PDF、文本、图像、音频等）
- **storage**：文件存储（本地、S3）
- **embeddings**：嵌入模型支持

#### 3.2.4 任务层（`cognee/tasks/`）
可重用的处理任务：
- **graph**：图谱提取任务
- **chunks**：分块任务
- **documents**：文档处理任务
- **summarization**：摘要生成
- **code**：代码分析任务
- **codingagents**：代码智能体任务
- **feedback**：反馈处理
- **temporal_awareness**：时间感知处理
- **web_scraper**：网页抓取
- **storage**：存储任务

---

## 四、数据源支持

### 4.1 文件类型支持

#### 核心加载器
- **TextLoader**：纯文本文件（.txt, .md, .csv 等）
- **PyPdfLoader**：PDF 文件
- **ImageLoader**：图像文件（支持 OCR）
- **AudioLoader**：音频文件（支持转录）

#### 可选加载器
- **UnstructuredLoader**：支持多种文档格式（需要安装 unstructured 包）
  - DOC, DOCX, EPUB, ODT, ORG, PPT, PPTX, RST, RTF, TSV, XLSX, PDF, MD, CSV
- **AdvancedPdfLoader**：高级 PDF 处理
- **BeautifulSoupLoader**：HTML 网页内容

### 4.2 数据源类型

- **本地文件系统**：直接读取本地文件
- **URL/网页**：抓取网页内容
- **代码仓库**：Git 仓库分析
- **数据库**：关系数据库模式导入
- **API 数据**：通过 API 获取数据
- **流式数据**：实时数据流处理

---

## 五、数据库支持

### 5.1 向量数据库

- **LanceDB**（默认）：轻量级、高性能向量数据库
- **ChromaDB**：开源向量数据库
- **PGVector**：PostgreSQL 扩展，支持向量搜索
- **Neptune Analytics**：AWS Neptune 分析服务

### 5.2 图数据库

- **Kuzu**（默认）：嵌入式图数据库，适合本地开发
- **Neo4j**：企业级图数据库
- **Neptune**：AWS 托管图数据库服务

### 5.3 关系数据库

- **PostgreSQL**：生产环境推荐
- **SQLite**：开发环境默认

### 5.4 缓存系统

- **Redis**：可选，用于缓存和会话管理

---

## 六、LLM 提供商支持

### 6.1 支持的提供商

- **OpenAI**：GPT-3.5, GPT-4, GPT-4 Turbo
- **Anthropic**：Claude 系列
- **Mistral AI**：Mistral 模型
- **Groq**：高速推理
- **Ollama**：本地模型部署
- **HuggingFace**：开源模型
- **其他**：通过 LiteLLM 支持 100+ 模型

### 6.2 嵌入模型

- **OpenAI**：text-embedding-3-small/large
- **FastEmbed**：本地嵌入模型（默认）
- **自定义**：支持任何兼容的嵌入模型

---

## 七、用户与权限系统

### 7.1 多租户架构

- **租户（Tenant）**：组织级别的数据隔离
- **租户编码**：6 位唯一编码，用于用户注册
- **租户有效期**：支持租户过期管理
- **租户所有者**：每个租户有唯一所有者

### 7.2 用户管理

- **用户注册**：支持邮箱注册，可关联租户
- **用户认证**：基于 FastAPI Users 的认证系统
- **用户角色**：多对多角色关系
- **用户权限**：基于 ACL 的细粒度权限控制

### 7.3 权限系统

- **数据集权限**：读、写、删除权限
- **角色权限**：基于角色的访问控制（RBAC）
- **ACL（访问控制列表）**：资源级别的权限控制
- **默认权限**：租户和用户级别的默认权限设置

### 7.4 API 密钥管理

- **租户级别 API Key**：支持程序化访问
- **密钥哈希存储**：安全存储
- **使用追踪**：记录最后使用时间
- **过期管理**：支持密钥过期设置

---

## 八、前端界面

### 8.1 技术栈

- **框架**：Next.js 15.3.3
- **UI 库**：React 19
- **样式**：Tailwind CSS
- **可视化**：D3.js, React Force Graph
- **国际化**：i18next

### 8.2 主要功能

- **数据管理界面**：上传、查看、管理数据
- **知识图谱可视化**：交互式图谱探索
- **搜索界面**：多种搜索模式的可视化界面
- **用户管理**：用户、租户、权限管理界面
- **设置配置**：系统配置和 LLM 设置

---

## 九、部署方式

### 9.1 自托管（开源版）

- **本地部署**：所有数据存储在本地
- **Docker 部署**：使用 docker-compose 一键部署
- **独立服务**：后端 API 和前端 UI 可独立部署

### 9.2 云端托管（Cognee Cloud）

- **托管服务**：由 Cognee 团队管理的云端服务
- **自动更新**：自动版本更新
- **资源监控**：资源使用分析
- **企业级安全**：GDPR 合规，企业级安全保障

### 9.3 MCP 服务器

- **Model Context Protocol**：支持 MCP 协议
- **传输方式**：支持 stdio、SSE、HTTP
- **工具暴露**：将 Cognee 功能暴露为 MCP 工具

---

## 十、使用场景

### 10.1 AI 智能体记忆

为 AI 智能体提供持久化记忆，支持：
- 对话历史记忆
- 知识库查询
- 上下文理解
- 个性化响应

### 10.2 企业知识管理

- 文档知识库构建
- 内部知识搜索
- 知识图谱可视化
- 团队协作知识共享

### 10.3 代码智能体

- 代码库分析
- 代码关系图谱
- 开发规则提取
- 代码搜索和理解

### 10.4 研究分析

- 学术论文分析
- 研究主题发现
- 文献关系网络
- 知识发现

### 10.5 内容管理

- 多媒体内容处理
- 内容关联分析
- 智能内容推荐
- 内容摘要生成

---

## 十一、核心工作流程

### 11.1 基本工作流

```python
import cognee
import asyncio

async def main():
    # 1. 添加数据
    await cognee.add("Cognee turns documents into AI memory.")
    
    # 2. 构建知识图谱
    await cognee.cognify()
    
    # 3. 增强记忆（可选）
    await cognee.memify()
    
    # 4. 搜索查询
    results = await cognee.search("What does Cognee do?")
    
    for result in results:
        print(result)

asyncio.run(main())
```

### 11.2 CLI 工作流

```bash
# 添加数据
cognee-cli add "Your data here"

# 构建图谱
cognee-cli cognify

# 搜索
cognee-cli search "Your question"

# 启动 UI
cognee-cli -ui
```

### 11.3 API 工作流

```bash
# 1. 添加数据
curl -X POST "http://localhost:8000/api/v1/add" \
  -H "Content-Type: application/json" \
  -d '{"data": "Your data", "dataset_name": "my_dataset"}'

# 2. 认知化处理
curl -X POST "http://localhost:8000/api/v1/cognify" \
  -H "Content-Type: application/json" \
  -d '{"datasets": ["my_dataset"]}'

# 3. 搜索
curl -X POST "http://localhost:8000/api/v1/search" \
  -H "Content-Type: application/json" \
  -d '{"query_text": "Your question", "query_type": "GRAPH_COMPLETION"}'
```

---

## 十二、高级特性

### 12.1 自定义图谱模型

支持定义领域特定的知识图谱结构：

```python
from cognee.infrastructure.engine.models import DataPoint

class ScientificPaper(DataPoint):
    title: str
    authors: List[str]
    methodology: str
    findings: List[str]

await cognee.cognify(
    datasets=["research_papers"],
    graph_model=ScientificPaper
)
```

### 12.2 本体集成

支持使用预定义的本体（OWL 文件）来指导实体和关系提取。

### 12.3 自定义任务

可以定义自定义的提取和增强任务，扩展 Cognee 的处理能力。

### 12.4 时间感知处理

支持时间相关的实体和事件提取，构建时间感知的知识图谱。

### 12.5 反馈循环

支持用户反馈，用于改进搜索结果和答案质量。

---

## 十三、性能与扩展

### 13.1 后台处理

支持大型数据集的后台异步处理，避免阻塞。

### 13.2 批量处理

支持批量处理数据，提高处理效率。

### 13.3 增量加载

支持增量数据加载，只处理新增或变更的数据。

### 13.4 分布式执行

支持通过 Modal 等平台进行分布式任务执行。

---

## 十四、安全与合规

### 14.1 数据安全

- 数据加密存储
- API 密钥安全管理
- 访问控制列表（ACL）
- 多租户数据隔离

### 14.2 合规性

- GDPR 合规
- 企业级安全标准
- 审计日志
- 数据保留策略

---

## 十五、监控与可观测性

### 15.1 日志系统

- 结构化日志（structlog）
- 日志文件管理
- 日志级别配置

### 15.2 遥测

- 操作追踪
- 性能监控
- 错误追踪（Sentry 集成）

### 15.3 评估框架

内置评估框架，支持：
- 检索质量评估
- 答案质量评估
- 性能基准测试

---

## 十六、评估框架详细说明

Cognee 内置了完整的评估框架（Evaluation Framework），用于系统性地评估知识图谱构建、检索和问答的质量。该框架支持端到端的评估流程，从语料库构建到答案生成，再到多维度指标评估。

### 16.1 评估框架架构

评估框架采用模块化设计，包含以下核心组件：

```
评估框架架构
├── 基准测试适配器（Benchmark Adapters）
│   ├── HotPotQA 适配器
│   ├── TwoWikiMultiHop 适配器
│   ├── MuSiQue 适配器
│   └── Dummy 适配器（测试用）
├── 语料库构建器（Corpus Builder）
│   ├── 数据加载
│   ├── Cognee 处理管道
│   └── 问题提取
├── 答案生成器（Answer Generator）
│   ├── 多种检索器支持
│   ├── 上下文检索
│   └── 答案生成
├── 评估执行器（Evaluation Executor）
│   ├── DeepEval 适配器
│   ├── DirectLLM 适配器
│   └── 自定义评估器
├── 评估指标（Metrics）
│   ├── Correctness（正确性）
│   ├── Exact Match（精确匹配）
│   ├── F1 Score（F1 分数）
│   ├── Contextual Relevancy（上下文相关性）
│   └── Context Coverage（上下文覆盖率）
└── 分析与可视化（Analysis & Visualization）
    ├── 指标计算
    ├── 置信区间分析
    └── 仪表板生成
```

### 16.2 评估流程

#### 16.2.1 完整评估流程

```python
# 1. 语料库构建阶段
await run_corpus_builder(eval_params)
# - 从基准测试数据集加载数据
# - 使用 Cognee 处理数据（add + cognify）
# - 提取问题和答案对

# 2. 答案生成阶段
await run_question_answering(eval_params)
# - 使用指定的检索器生成答案
# - 记录检索上下文
# - 保存答案和上下文

# 3. 评估阶段
await run_evaluation(eval_params)
# - 使用评估引擎评估答案质量
# - 计算各项指标
# - 生成评估报告

# 4. 可视化阶段（可选）
create_dashboard(...)
# - 生成指标分布图
# - 计算置信区间
# - 创建交互式仪表板
```

### 16.3 配置参数（EvalConfig）

评估框架通过 `EvalConfig` 类进行配置，支持以下参数：

#### 16.3.1 语料库构建参数

- **building_corpus_from_scratch** (bool): 是否从头构建语料库
- **number_of_samples_in_corpus** (int): 语料库中的样本数量
- **benchmark** (str): 基准测试数据集
  - `"HotPotQA"`: 多跳问答基准测试
  - `"TwoWikiMultiHop"`: 双维基多跳问答
  - `"MuSiQue"`: 多源问答数据集
  - `"Dummy"`: 测试用虚拟数据集
- **task_getter_type** (str): 任务获取器类型
  - `"Default"`: 默认任务管道
  - `"CascadeGraph"`: 级联图谱任务
  - `"NoSummaries"`: 无摘要任务
  - `"JustChunks"`: 仅分块任务
- **instance_filter** (Optional[List[str]]): 实例过滤器，用于选择特定问题

#### 16.3.2 问答生成参数

- **answering_questions** (bool): 是否执行问答生成
- **qa_engine** (str): 问答引擎类型
  - `"cognee_completion"`: 标准完成检索器
  - `"cognee_graph_completion"`: 图谱完成检索器
  - `"cognee_graph_completion_cot"`: 带思维链的图谱完成检索器
  - `"cognee_graph_completion_context_extension"`: 上下文扩展图谱完成检索器
  - `"graph_summary_completion"`: 图谱摘要完成检索器

#### 16.3.3 评估参数

- **evaluating_answers** (bool): 是否评估答案
- **evaluating_contexts** (bool): 是否评估上下文
- **evaluation_engine** (str): 评估引擎
  - `"DeepEval"`: 使用 DeepEval 框架（推荐）
  - `"DirectLLM"`: 直接使用 LLM 评估
- **evaluation_metrics** (List[str]): 评估指标列表
  - `"correctness"`: 正确性评估（LLM 判断）
  - `"EM"`: 精确匹配（Exact Match）
  - `"f1"`: F1 分数
  - `"contextual_relevancy"`: 上下文相关性（需要 evaluating_contexts=True）
  - `"context_coverage"`: 上下文覆盖率（需要 evaluating_contexts=True）
- **deepeval_model** (str): DeepEval 使用的模型（如 "gpt-4", "gpt-5-mini"）

#### 16.3.4 文件路径配置

- **questions_path**: 问题输出文件路径
- **answers_path**: 答案输出文件路径
- **metrics_path**: 指标输出文件路径
- **aggregate_metrics_path**: 聚合指标输出文件路径
- **dashboard_path**: 仪表板 HTML 文件路径
- **direct_llm_system_prompt**: DirectLLM 评估的系统提示词文件
- **direct_llm_eval_prompt**: DirectLLM 评估的提示词文件

### 16.4 基准测试适配器

#### 16.4.1 HotPotQA 适配器

HotPotQA 是一个多跳问答基准测试，要求系统整合多个文档的信息来回答问题。

**特点：**
- 支持从 URL 或本地文件加载数据
- 自动提取支持事实（supporting facts）作为黄金上下文
- 支持实例过滤
- 支持随机采样

**使用示例：**
```python
from cognee.eval_framework.benchmark_adapters.hotpot_qa_adapter import HotpotQAAdapter

adapter = HotpotQAAdapter()
corpus, questions = adapter.load_corpus(
    limit=24,  # 限制问题数量
    load_golden_context=True,  # 加载黄金上下文
    instance_filter=["question_id_1", "question_id_2"]  # 过滤特定问题
)
```

#### 16.4.2 自定义基准测试适配器

可以通过继承 `BaseBenchmarkAdapter` 创建自定义适配器：

```python
from cognee.eval_framework.benchmark_adapters.base_benchmark_adapter import BaseBenchmarkAdapter

class CustomBenchmarkAdapter(BaseBenchmarkAdapter):
    def load_corpus(
        self,
        limit: Optional[int] = None,
        seed: int = 42,
        load_golden_context: bool = False,
        instance_filter: Optional[Union[str, List[str], List[int]]] = None,
    ) -> Tuple[List[str], List[dict[str, Any]]]:
        # 实现自定义数据加载逻辑
        corpus = [...]  # 语料库数据
        questions = [...]  # 问题答案对
        return corpus, questions
```

### 16.5 评估指标详解

#### 16.5.1 Correctness（正确性）

**描述：** 使用 LLM 判断答案的事实正确性，考虑与标准答案的一致性。

**评估方式：**
- DeepEval: 使用 GEval 框架，基于多步骤评估
- DirectLLM: 直接使用配置的 LLM 进行评估

**评估步骤（DeepEval）：**
1. 检查实际输出中的事实是否与期望输出矛盾
2. 严重惩罚细节遗漏
3. 允许模糊语言或不同观点

**输出：**
- `score`: 0.0-1.0 的分数
- `reason`: 评估理由

#### 16.5.2 Exact Match (EM)

**描述：** 精确匹配指标，检查答案是否与标准答案完全一致（忽略大小写和空格）。

**计算方式：**
```python
actual = answer.strip().lower()
expected = golden_answer.strip().lower()
score = 1.0 if actual == expected else 0.0
```

**适用场景：** 适合有明确标准答案的问题（如日期、数字、名称等）。

#### 16.5.3 F1 Score

**描述：** 基于词级别的 F1 分数，衡量答案与标准答案的重叠程度。

**计算方式：**
1. 将答案和标准答案分词并去除标点
2. 计算精确率（Precision）和召回率（Recall）
3. F1 = 2 * (Precision * Recall) / (Precision + Recall)

**优势：** 比精确匹配更灵活，能处理同义词和不同表达方式。

#### 16.5.4 Contextual Relevancy（上下文相关性）

**描述：** 评估检索到的上下文与问题的相关性。

**评估方式：** 使用 DeepEval 的 ContextualRelevancyMetric，判断检索上下文是否包含回答问题所需的信息。

**适用场景：** 评估检索质量，而非答案质量。

#### 16.5.5 Context Coverage（上下文覆盖率）

**描述：** 评估检索上下文对标准上下文的覆盖程度。

**计算方式：** 基于摘要指标（SummarizationMetric），比较检索上下文与标准上下文的信息覆盖度。

**输出：** 0.0-1.0 的分数，表示覆盖率。

### 16.6 评估引擎

#### 16.6.1 DeepEval 适配器

**特点：**
- 支持多种预定义指标
- 使用 GEval 进行 LLM 评估
- 支持异步评估
- 自动重试机制（默认 5 次）

**支持的指标：**
- Correctness（通过 GEval）
- Exact Match
- F1 Score
- Contextual Relevancy
- Context Coverage

**配置示例：**
```python
evaluation_engine = "DeepEval"
evaluation_metrics = ["correctness", "EM", "f1"]
deepeval_model = "gpt-4"  # 或 "gpt-5-mini"
```

#### 16.6.2 DirectLLM 适配器

**特点：**
- 直接使用配置的 LLM（从 .env 读取）
- 支持自定义提示词
- 仅支持 Correctness 指标
- 更灵活但需要手动设计提示词

**配置示例：**
```python
evaluation_engine = "DirectLLM"
evaluation_metrics = ["correctness"]  # 仅支持 correctness
direct_llm_system_prompt = "direct_llm_eval_system.txt"
direct_llm_eval_prompt = "direct_llm_eval_prompt.txt"
```

### 16.7 指标分析与可视化

#### 16.7.1 指标统计计算

评估框架自动计算以下统计信息：

- **均值（Mean）**: 所有样本的平均分数
- **置信区间（Confidence Interval）**: 使用 Bootstrap 方法计算 95% 置信区间
- **分布分析**: 生成分数分布直方图

**Bootstrap 方法：**
```python
# 默认使用 10000 次重采样
mean, lower_bound, upper_bound = bootstrap_ci(scores, num_samples=10000, confidence_level=0.95)
```

#### 16.7.2 仪表板生成

评估框架自动生成交互式 HTML 仪表板，包含：

1. **指标分布图**
   - 每个指标的分数分布直方图
   - 使用 Plotly 生成交互式图表

2. **置信区间图**
   - 所有指标的均值及 95% 置信区间
   - 条形图展示，带误差线

3. **详细评估表**
   - 每个问题的详细评估结果
   - 包含问题、答案、标准答案、评估理由和分数

**仪表板特点：**
- 完全自包含的 HTML 文件
- 使用 Plotly 进行交互式可视化
- 响应式设计，适合不同屏幕尺寸
- 可直接在浏览器中打开查看

### 16.8 使用示例

#### 16.8.1 基本使用

```python
from cognee.eval_framework.run_eval import main
import asyncio

# 使用默认配置运行评估
asyncio.run(main())
```

#### 16.8.2 自定义配置

```python
from cognee.eval_framework.eval_config import EvalConfig
from cognee.eval_framework.run_eval import main
import asyncio

# 创建自定义配置
config = EvalConfig(
    benchmark="HotPotQA",
    number_of_samples_in_corpus=24,
    qa_engine="cognee_graph_completion_cot",
    evaluation_engine="DeepEval",
    evaluation_metrics=["correctness", "EM", "f1"],
    evaluating_contexts=True,
    deepeval_model="gpt-4",
    dashboard=True
)

# 转换为字典并运行
eval_params = config.to_dict()
# 修改 run_eval.py 使用自定义参数，或直接调用各个模块
```

#### 16.8.3 分布式评估（Modal）

对于大规模评估，可以使用 Modal 进行分布式执行：

```python
from cognee.eval_framework.modal_run_eval import modal_run_eval

# 在 Modal 上运行评估
await modal_run_eval(eval_params)
```

### 16.9 评估结果解读

#### 16.9.1 指标分数含义

- **Correctness**: 
  - 0.8-1.0: 答案基本正确，可能有轻微遗漏
  - 0.5-0.8: 答案部分正确，存在重要信息缺失
  - 0.0-0.5: 答案不正确或严重偏离

- **Exact Match**: 
  - 1.0: 完全匹配
  - 0.0: 不匹配

- **F1 Score**: 
  - 0.9-1.0: 高度重叠
  - 0.7-0.9: 良好重叠
  - 0.5-0.7: 中等重叠
  - <0.5: 低重叠

#### 16.9.2 置信区间解读

置信区间反映了评估结果的稳定性：
- **窄区间**: 结果稳定，评估一致性好
- **宽区间**: 结果波动大，可能需要更多样本

### 16.10 最佳实践

1. **选择合适的基准测试**
   - HotPotQA: 适合多跳推理场景
   - TwoWikiMultiHop: 适合跨文档推理
   - 自定义数据集: 适合特定领域评估

2. **组合使用多个指标**
   - Correctness: 评估事实准确性
   - F1: 评估信息完整性
   - Context Coverage: 评估检索质量

3. **评估上下文质量**
   - 启用 `evaluating_contexts=True`
   - 使用 Contextual Relevancy 和 Context Coverage
   - 帮助诊断检索问题

4. **多次运行取平均**
   - LLM 评估存在随机性
   - 建议运行多次并计算平均分数
   - 使用置信区间评估稳定性

5. **分析失败案例**
   - 查看仪表板中的详细评估表
   - 分析低分问题的原因
   - 针对性改进检索或生成策略

### 16.11 评估框架的优势

1. **模块化设计**: 各组件可独立使用和扩展
2. **多指标支持**: 从多个维度评估系统性能
3. **可视化分析**: 自动生成交互式仪表板
4. **统计严谨**: 使用 Bootstrap 方法计算置信区间
5. **易于扩展**: 支持自定义基准测试和评估指标
6. **分布式支持**: 支持 Modal 等平台进行大规模评估

### 16.12 已知限制

1. **LLM 评估的随机性**: 使用 LLM 作为评估器存在一定随机性，建议多次运行
2. **传统指标局限性**: EM/F1 等传统指标可能无法完全反映 AI 记忆系统的价值
3. **基准测试适配性**: 某些基准测试（如 HotPotQA）可能不完全适合评估跨上下文链接场景
4. **计算成本**: 使用 LLM 评估会产生 API 调用成本

---

## 十七、社区与支持

---

## 十六、社区与支持

### 16.1 开源社区

- **GitHub**：主要代码仓库
- **Discord**：社区讨论
- **Reddit**：r/AIMemory 社区
- **文档**：https://docs.cognee.ai

### 16.2 贡献

- 欢迎社区贡献
- 代码规范：Ruff 格式化
- 测试要求：单元测试和集成测试
- 提交指南：遵循约定式提交

---

## 十七、总结

Cognee 是一个功能强大、高度可定制的 AI 记忆平台，通过结合向量搜索和图数据库技术，为 AI 智能体提供了强大的知识记忆和推理能力。无论是个人开发者还是企业用户，都可以通过 Cognee 构建自己的智能知识系统。

### 核心优势

1. **统一架构**：用 ECL 管道替代传统 RAG，提供更强大的能力
2. **高度可定制**：支持自定义模型、任务、加载器等
3. **多数据源**：支持 30+ 数据源和多种文件格式
4. **企业级**：多租户、权限管理、安全合规
5. **易于使用**：简单的 API、CLI 和 Web UI
6. **开源免费**：Apache-2.0 许可证，完全开源

### 适用对象

- AI 智能体开发者
- 企业知识管理团队
- 研究机构
- 内容管理平台
- 代码智能体开发者

---

**文档版本**：1.0  
**最后更新**：2025-12-25  
**基于 Cognee 版本**：0.4.1

